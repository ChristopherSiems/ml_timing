{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\csiems\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\csiems\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\csiems\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from json import dumps, load\n",
    "from pathlib import Path\n",
    "from timeit import default_timer\n",
    "from typing import Callable, Dict\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from numpy import append, array, float64, ndarray, reshape, uint8, unique\n",
    "from numpy.ma import masked_array, masked_less_equal\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer():\n",
    "  '''from given starter code'''\n",
    "\n",
    "  start = default_timer()\n",
    "  def elapser(): return default_timer() - start\n",
    "  yield lambda: elapser()\n",
    "  end = default_timer()\n",
    "  def elapser(): return end - start\n",
    "\n",
    "\n",
    "def time(name: str, model: OneVsRestClassifier | SGDClassifier | SVC, X: NDArray[float64], y: NDArray[uint8]) -> None:\n",
    "  '''\n",
    "  time fitting models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param model: the model to fit\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param X: the training feature set\n",
    "  :type X: NDArray[float64]\n",
    "  :param y: the training label set\n",
    "  :type y: NDArray[uint8]\n",
    "  '''\n",
    "\n",
    "  file: str = f'times/{name}.txt'\n",
    "  if Path(file).is_file():\n",
    "    with open(file, 'r') as time:\n",
    "      print(f'\\ntraining time: {time.read()}')\n",
    "  else:\n",
    "    with timer() as fit_timer:\n",
    "      model.fit(X, y)\n",
    "\n",
    "    curr_time: float = fit_timer()\n",
    "    print(f'training time: {curr_time}')\n",
    "    with open(file, 'w') as time:\n",
    "      time.write(str(curr_time))\n",
    "\n",
    "def crop_time(name: str, X_train: NDArray[float64], X_test: NDArray[float64], model: OneVsRestClassifier | SGDClassifier | SVC, y_train: NDArray[uint8], y_test: NDArray[uint8]) -> None:\n",
    "  '''\n",
    "  time fitting cropped models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param X_train: the feature set to train with\n",
    "  :type X_train: NDArray[float64]\n",
    "  :param X_test: the feature set to test with\n",
    "  :type X_test: NDArray[float64]\n",
    "  :param model: the model to train\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param y_train: the training labels\n",
    "  :type y_train: NDArray[uint8]\n",
    "  :param y_test: the testing labels\n",
    "  :type y_test: NDArray[uint8]\n",
    "  '''\n",
    "\n",
    "  crop: Callable[[NDArray[float64], int], NDArray[float64]] = lambda X, dim: array([instance.reshape(28, 28)[i:dim, i:dim].flatten() for instance in X])\n",
    "  info: Dict[str, int | float] = {'dim': 28, 'time': float('inf'), 'perf': 0}\n",
    "  file: str = f'times/{name}.json'\n",
    "  if Path(file).is_file():\n",
    "    with open(file, 'r') as json_file:\n",
    "      info = load(json_file)\n",
    "    print(f'\\ndim.: {info['dim']}\\ntraining time: {info['time']}\\nf1-score: {info['perf']}')\n",
    "  else:\n",
    "    for i in range(1, 14):\n",
    "      curr_dim: int = 28 - i\n",
    "      X_train_crop: NDArray[float64] = crop(X_train, curr_dim)\n",
    "      X_test_crop: NDArray[float64] = crop(X_test, curr_dim)\n",
    "\n",
    "      with timer() as fit_timer:\n",
    "        model.fit(X_train_crop, y_train)\n",
    "      score: float = f1_score(y_test, model.predict(X_test_crop), average='micro')\n",
    "      curr_time: float = fit_timer()\n",
    "      print(f'\\ndim.: {curr_dim}\\ntraining time: {curr_time}\\nf1-score: {score}')\n",
    "      sgd: SGDClassifier = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "      if score < info['perf']:\n",
    "        break\n",
    "      info = {'dim': curr_dim, 'time': curr_time, 'perf': score}\n",
    "\n",
    "    with open(file, 'w') as json_file:\n",
    "      json_file.write(dumps(info, indent=2))\n",
    "\n",
    "def importance_time(name: str, importances_unique: NDArray[float64], importances: NDArray[float64], X_train: NDArray[float64], X_test: NDArray[float64], model: OneVsRestClassifier | SGDClassifier | SVC, y_train: NDArray[float64], y_test: NDArray[float64]) -> None:\n",
    "  '''\n",
    "  time fitting importance models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param importances_unique: the unique importance values\n",
    "  :type importances_unique: NDArray[float64]\n",
    "  :param importances: the importances of each feature\n",
    "  :type importances: NDArray[float64]\n",
    "  :param X_train: the training feature set\n",
    "  :type X_train: NDArray[float64]\n",
    "  :param X_test: the training feature set\n",
    "  :type X_test: NDArray[float64]\n",
    "  :param model: the model to train\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param y_train: the training labels\n",
    "  :type y_train: NDArray[float64]\n",
    "  :param y_test: the testing labels\n",
    "  :type y_test: NDArray[float64]\n",
    "  '''\n",
    "\n",
    "  mask_array: Callable[[NDArray[bool], NDArray[float64]], NDArray[float64]] = lambda mask, X: array([masked_array(instance, mask).compressed() for instance in X])\n",
    "  info: Dict[str, float] = {'thresh': 0, 'time': float('inf'), 'perf': 0}\n",
    "  file: str = f'times/{name}.json'\n",
    "  if Path(file).is_file():\n",
    "    with open(file, 'r') as json_file:\n",
    "      info = load(json_file)\n",
    "    print(f'\\nthresh.: {info['thresh']}\\ntraining time: {info['time']}\\nf1-score: {info['perf']}')\n",
    "  else:\n",
    "    for thresh in importances_unique:\n",
    "      mask: NDArray[bool] = masked_less_equal(importances, thresh).mask\n",
    "      X_train_mask: NDArray[float64] = masked_array(mask, X_train)\n",
    "      X_test_mask: NDArray[float64] = masked_array(mask, X_test)\n",
    "\n",
    "      with timer() as fit_timer:\n",
    "        model.fit(X_train_mask, y_train)\n",
    "      score: float = f1_score(y_test, model.predict(X_test_mask), average='micro')\n",
    "      curr_time: float = fit_timer()\n",
    "      print(f'\\nthresh.: {thresh}\\ntraining time: {curr_time}\\nf1-score: {score}')\n",
    "\n",
    "      if score < info['perf']:\n",
    "        break\n",
    "      info = {'thresh': thresh, 'time': curr_time, 'perf': score}\n",
    "\n",
    "    with open(file, 'w') as json_file:\n",
    "      json_file.write(dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist: DataFrame = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "X: NDArray[float64] = mnist['data'].astype(float64).to_numpy()\n",
    "y: NDArray[uint8] = mnist['target'].astype(uint8).to_numpy()\n",
    "\n",
    "X_train: NDArray[float64]\n",
    "X_test: NDArray[float64]\n",
    "X_train = X[:60000]\n",
    "X_test = X[60000:]\n",
    "\n",
    "y_train: NDArray[uint8]\n",
    "y_test: NDArray[uint8]\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]\n",
    "\n",
    "scaler: StandardScaler = StandardScaler()\n",
    "X_train_scale: NDArray[float64] = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 62.35176499999943\n"
     ]
    }
   ],
   "source": [
    "time('sgd', SGDClassifier(random_state=42, n_jobs=-1), X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 0.14785229999688454\n"
     ]
    }
   ],
   "source": [
    "time('svc_1000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 0.4460183000046527\n"
     ]
    }
   ],
   "source": [
    "time('svc_2000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 1.270653300001868\n"
     ]
    }
   ],
   "source": [
    "time('svc_4000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:4000], y_train[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 6.220961700004409\n"
     ]
    }
   ],
   "source": [
    "time('ovr_1000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 3.7742016999982297\n"
     ]
    }
   ],
   "source": [
    "time('ovr_2000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 1.4222129999980098\n"
     ]
    }
   ],
   "source": [
    "time('ovr_4000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:4000], y_train[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 15\n",
      "training time: 34.4232697999978\n",
      "f1-score: 0.874\n"
     ]
    }
   ],
   "source": [
    "crop_time('sgd_crop', X_train, X_test, SGDClassifier(random_state=42, n_jobs=-1), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 15\n",
      "training time: 0.9878120999928797\n",
      "f1-score: 0.2346\n"
     ]
    }
   ],
   "source": [
    "crop_time('svc_crop', X_train[:4000], X_test, SVC(gamma='auto', random_state=42), y_train[:4000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 16\n",
      "training time: 0.9502957999939099\n",
      "f1-score: 0.1883\n"
     ]
    }
   ],
   "source": [
    "crop_time('ovr_crop', X_train[:4000], X_test, OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1), y_train[:4000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest: RandomForestClassifier = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances: NDArray[float64] = forest.feature_importances_\n",
    "importances_unique: NDArray[float64] = unique(importances)\n",
    "importances_unique.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "thresh.: 7.203407876315988e-08\n",
      "training time: 24.88607789998059\n",
      "f1-score: 0.8973\n"
     ]
    }
   ],
   "source": [
    "importance_time('sgd_important', importances_unique, importances, X_train, X_test, SGDClassifier(random_state=42, n_jobs=-1), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [60000, 4000]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mimportance_time\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msvc_important\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances_unique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mimportance_time\u001b[39m\u001b[34m(name, importances_unique, importances, X_train, X_test, model, y_train, y_test)\u001b[39m\n\u001b[32m    115\u001b[39m X_test_mask = X_mask[\u001b[32m60000\u001b[39m:]\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m timer() \u001b[38;5;28;01mas\u001b[39;00m fit_timer:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m   \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m score: \u001b[38;5;28mfloat\u001b[39m = f1_score(y_test, model.predict(X_test_mask), average=\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    120\u001b[39m curr_time: \u001b[38;5;28mfloat\u001b[39m = fit_timer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csiems\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csiems\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:197\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    195\u001b[39m     check_consistent_length(X, y)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m y = \u001b[38;5;28mself\u001b[39m._validate_targets(y)\n\u001b[32m    209\u001b[39m sample_weight = np.asarray(\n\u001b[32m    210\u001b[39m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype=np.float64\n\u001b[32m    211\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csiems\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csiems\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1389\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1370\u001b[39m X = check_array(\n\u001b[32m   1371\u001b[39m     X,\n\u001b[32m   1372\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1384\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1385\u001b[39m )\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\csiems\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [60000, 4000]"
     ]
    }
   ],
   "source": [
    "importance_time('svc_important', importances_unique, importances, X_train[:4000], X_test, SVC(gamma='auto', random_state=42), y_train[:4000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_time('ovr_important', importances_unique, importances, X_train[:4000], X_test, OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1), y_train[:4000], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
