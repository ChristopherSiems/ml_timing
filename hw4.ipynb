{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./env/lib64/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in ./env/lib64/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib64/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib64/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib64/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib64/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib64/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib64/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from json import dumps, load\n",
    "from pathlib import Path\n",
    "from timeit import default_timer\n",
    "from typing import Callable, Dict\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from numpy import append, array, float64, ndarray, reshape, uint8, unique\n",
    "from numpy.ma import masked_array, masked_less_equal\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer():\n",
    "  '''from given starter code'''\n",
    "\n",
    "  start = default_timer()\n",
    "  def elapser(): return default_timer() - start\n",
    "  yield lambda: elapser()\n",
    "  end = default_timer()\n",
    "  def elapser(): return end - start\n",
    "\n",
    "\n",
    "def time(name: str, model: OneVsRestClassifier | SGDClassifier | SVC, X: NDArray[float64], y: NDArray[uint8]) -> None:\n",
    "  '''\n",
    "  time fitting models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param model: the model to fit\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param X: the training feature set\n",
    "  :type X: NDArray[float64]\n",
    "  :param y: the training label set\n",
    "  :type y: NDArray[uint8]\n",
    "  '''\n",
    "\n",
    "  file: str = f'times/{name}.txt'\n",
    "  if Path(file).is_file():\n",
    "    with open(file, 'r') as time:\n",
    "      print(f'\\ntraining time: {time.read()}')\n",
    "  else:\n",
    "    with timer() as fit_timer:\n",
    "      model.fit(X, y)\n",
    "\n",
    "    curr_time: float = fit_timer()\n",
    "    print(f'training time: {curr_time}')\n",
    "    with open(file, 'w') as time:\n",
    "      time.write(str(curr_time))\n",
    "\n",
    "def crop_time(name: str, X_train: NDArray[float64], X_test: NDArray[float64], model: OneVsRestClassifier | SGDClassifier | SVC, y_train: NDArray[uint8], y_test: NDArray[uint8]) -> None:\n",
    "  '''\n",
    "  time fitting cropped models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param X_train: the feature set to train with\n",
    "  :type X_train: NDArray[float64]\n",
    "  :param X_test: the feature set to test with\n",
    "  :type X_test: NDArray[float64]\n",
    "  :param model: the model to train\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param y_train: the training labels\n",
    "  :type y_train: NDArray[uint8]\n",
    "  :param y_test: the testing labels\n",
    "  :type y_test: NDArray[uint8]\n",
    "  '''\n",
    "\n",
    "  crop: Callable[[NDArray[float64], int], NDArray[float64]] = lambda X, dim: array([instance.reshape(28, 28)[i:dim, i:dim].flatten() for instance in X])\n",
    "  info: Dict[str, int | float] = {'dim': 28, 'time': float('inf'), 'perf': 0}\n",
    "  file: str = f'times/{name}.json'\n",
    "\n",
    "  if Path(file).is_file():\n",
    "    with open(file, 'r') as json_file:\n",
    "      info = load(json_file)\n",
    "    print(f'\\ndim.: {info['dim']}\\ntraining time: {info['time']}\\nf1-score: {info['perf']}')\n",
    "  else:\n",
    "    for i in range(1, 14):\n",
    "      curr_dim: int = 28 - i\n",
    "      X_train_crop: NDArray[float64] = crop(X_train, curr_dim)\n",
    "      X_test_crop: NDArray[float64] = crop(X_test, curr_dim)\n",
    "\n",
    "      with timer() as fit_timer:\n",
    "        model.fit(X_train_crop, y_train)\n",
    "      score: float = f1_score(y_test, model.predict(X_test_crop), average='micro')\n",
    "      curr_time: float = fit_timer()\n",
    "      print(f'\\ndim.: {curr_dim}\\ntraining time: {curr_time}\\nf1-score: {score}')\n",
    "      sgd: SGDClassifier = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "      if score < info['perf']:\n",
    "        break\n",
    "      info = {'dim': curr_dim, 'time': curr_time, 'perf': score}\n",
    "\n",
    "    with open(file, 'w') as json_file:\n",
    "      json_file.write(dumps(info, indent=2))\n",
    "\n",
    "def importance_time(name: str, importances_unique: NDArray[float64], importances: NDArray[float64], X: NDArray[float64], model: OneVsRestClassifier | SGDClassifier | SVC, y_train: NDArray[float64], y_test: NDArray[float64]) -> None:\n",
    "  '''\n",
    "  time fitting importance models\n",
    "  :param name: the name of the model\n",
    "  :type name: str\n",
    "  :param importances_unique: the unique importance values\n",
    "  :type importances_unique: NDArray[float64]\n",
    "  :param importances: the importances of each feature\n",
    "  :type importances: NDArray[float64]\n",
    "  :param X: the feature set\n",
    "  :type X: NDArray[float64]\n",
    "  :param model: the model to train\n",
    "  :type model: OneVsRestClassifier | SGDClassifier | SVC\n",
    "  :param y_train: the training labels\n",
    "  :type y_train: NDArray[float64]\n",
    "  :param y_test: the testing labels\n",
    "  :type y_test: NDArray[float64]\n",
    "  '''\n",
    "\n",
    "  info: Dict[str, float] = {'thresh': 0, 'time': float('inf'), 'perf': 0}\n",
    "\n",
    "  if Path(f'times/{name}.json').is_file():\n",
    "    with open(file, 'r') as json_file:\n",
    "      info = load(json_file)\n",
    "    print(f'\\nthresh.: {info['thresh']}\\ntraining time: {info['time']}\\nf1-score: {info['perf']}')\n",
    "  else:\n",
    "    for thresh in importances_unique:\n",
    "      mask: NDArray[bool] = masked_less_equal(importances, thresh).mask\n",
    "      X_mask: NDArray[float64] = array([masked_array(instance, mask).compressed() for instance in X])\n",
    "      X_train_mask: NDArray[float64]\n",
    "      X_test_mask: NDArray[float64]\n",
    "      X_train_mask = X_mask[:60000]\n",
    "      X_test_mask = X_mask[60000:]\n",
    "\n",
    "      with timer() as fit_timer:\n",
    "        model.fit(X_train_mask, y_train)\n",
    "      score: float = f1_score(y_test, model.predict(X_test_mask), average='micro')\n",
    "      curr_time: float = fit_timer()\n",
    "      print(f'\\nthresh.: {thresh}\\ntraining time: {curr_time}\\nf1-score: {score}')\n",
    "\n",
    "      if score < info['perf']:\n",
    "        break\n",
    "      info = {'thresh': thresh, 'time': curr_time, 'perf': score}\n",
    "\n",
    "    with open(file, 'w') as json_file:\n",
    "      json_file.write(dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist: DataFrame = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "X: NDArray[float64] = mnist['data'].astype(float64).to_numpy()\n",
    "y: NDArray[uint8] = mnist['target'].astype(uint8).to_numpy()\n",
    "\n",
    "X_train: NDArray[float64]\n",
    "X_test: NDArray[float64]\n",
    "X_train = X[:60000]\n",
    "X_test = X[60000:]\n",
    "\n",
    "y_train: NDArray[uint8]\n",
    "y_test: NDArray[uint8]\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]\n",
    "\n",
    "scaler: StandardScaler = StandardScaler()\n",
    "X_train_scale: NDArray[float64] = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 62.35176499999943\n"
     ]
    }
   ],
   "source": [
    "time('sgd', SGDClassifier(random_state=42, n_jobs=-1), X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 0.14785229999688454\n"
     ]
    }
   ],
   "source": [
    "time('svc_1000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 0.4460183000046527\n"
     ]
    }
   ],
   "source": [
    "time('svc_2000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 1.270653300001868\n"
     ]
    }
   ],
   "source": [
    "time('svc_4000', SVC(gamma='auto', random_state=42),\n",
    "     X_train_scale[:4000], y_train[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 6.220961700004409\n"
     ]
    }
   ],
   "source": [
    "time('ovr_1000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 3.7742016999982297\n"
     ]
    }
   ],
   "source": [
    "time('ovr_2000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training time: 1.4222129999980098\n"
     ]
    }
   ],
   "source": [
    "time('ovr_4000', OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1),\n",
    "     X_train_scale[:4000], y_train[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 15\n",
      "training time: 34.4232697999978\n",
      "f1-score: 0.874\n"
     ]
    }
   ],
   "source": [
    "crop_time('sgd_crop', X_train, X_test, SGDClassifier(random_state=42, n_jobs=-1), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 15\n",
      "training time: 0.9878120999928797\n",
      "f1-score: 0.2346\n"
     ]
    }
   ],
   "source": [
    "crop_time('svc_crop', X_train[:4000], X_test, SVC(gamma='auto', random_state=42), y_train[:4000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim.: 16\n",
      "training time: 0.9502957999939099\n",
      "f1-score: 0.1883\n"
     ]
    }
   ],
   "source": [
    "crop_time('ovr_crop', X_train[:4000], X_test, OneVsRestClassifier(SVC(gamma='auto', random_state=42), n_jobs=-1), y_train[:4000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest: RandomForestClassifier = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances: NDArray[float64] = forest.feature_importances_\n",
    "importances_unique: NDArray[float64] = unique(importances)\n",
    "importances_unique.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_time('sgd_important', importances_unique, importances, X, SGDClassifier(random_state=42, n_jobs=-1), y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
